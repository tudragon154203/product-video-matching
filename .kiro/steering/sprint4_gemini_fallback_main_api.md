# Sprint 4 ‚Äî Fallback Gemini cho main-api

## üåü M·ª•c ti√™u

1. B·ªï sung bi·∫øn m√¥i tr∆∞·ªùng cho **Gemini** v√†o `.env` c·ªßa `services/main-api`:
   - `GEMINI_API_KEY`
   - `GEMINI_MODEL`
2. ƒê·ªïi t√™n bi·∫øn `OLLAMA_TIMEOUT` th√†nh `LLM_TIMEOUT` (d√πng chung cho c·∫£ Ollama v√† Gemini).
3. Tri·ªÉn khai **fallback**: n·∫øu Ollama b·ªã timeout ho·∫∑c l·ªói, t·ª± ƒë·ªông chuy·ªÉn sang g·ªçi Gemini.

---

## üß¨ Ph·∫°m vi & Deliverables

- C·∫≠p nh·∫≠t **services/main-api/.env.example** (v√† `.env` n·∫øu c√≥) v·ªõi `GEMINI_API_KEY`, `GEMINI_MODEL`, `LLM_TIMEOUT`.
- C·∫≠p nh·∫≠t **config\_loader** ƒë·ªÉ ƒë·ªçc c√°c bi·∫øn m·ªõi.
- Th√™m **client Gemini** ƒë∆°n gi·∫£n (HTTP) + h√†m `call_gemini()`.
- B·ªï sung \*\*h√†m wrapper \*\*\`\`: th·ª≠ Ollama tr∆∞·ªõc, n·∫øu l·ªói/timeout ‚Üí fallback sang Gemini.
- C·∫≠p nh·∫≠t lu·ªìng g·ªçi model trong `main-api` sang d√πng `call_llm()`.
- B·ªï sung test cho fallback (mock timeout Ollama, k·ª≥ v·ªçng g·ªçi Gemini).

---

## üõ†Ô∏è Thi·∫øt k·∫ø ng·∫Øn g·ªçn

### 1) Bi·∫øn m√¥i tr∆∞·ªùng

**services/main-api/.env.example**:

```env
# Gemini
GEMINI_API_KEY=
GEMINI_MODEL=gemini-1.5-flash

# LLM Timeout (seconds)
LLM_TIMEOUT=60
```

### 2) ƒê·ªçc c·∫•u h√¨nh

**config\_loader.py**:

```python
@dataclass
class MainAPIConfig:
    # Ollama
    OLLAMA_HOST: str = os.getenv("OLLAMA_HOST", "http://host.docker.internal:11434")
    OLLAMA_MODEL_CLASSIFY: str = os.getenv("OLLAMA_MODEL_CLASSIFY", "qwen3:4b-instruct")
    OLLAMA_MODEL_GENERATE: str = os.getenv("OLLAMA_MODEL_GENERATE", "qwen3:4b-instruct")

    # LLM chung
    LLM_TIMEOUT: int = int(os.getenv("LLM_TIMEOUT", "60"))

    # Gemini
    GEMINI_API_KEY: str = os.getenv("GEMINI_API_KEY", "")
    GEMINI_MODEL: str = os.getenv("GEMINI_MODEL", "gemini-1.5-flash")
```

### 3) L·ªõp g·ªçi LLM

**main.py**:

```python
async def call_gemini(model: str, prompt: str, timeout_s: int, **kwargs) -> dict:
    if not config.GEMINI_API_KEY:
        raise RuntimeError("GEMINI_API_KEY is not set")
    headers = {"Authorization": f"Bearer {config.GEMINI_API_KEY}"}
    payload = {"model": model, "input": prompt}
    async with httpx.AsyncClient(timeout=timeout_s) as client:
        r = await client.post("https://generativelanguage.googleapis.com/v1beta/models:generateContent",
                              headers=headers, json=payload)
        r.raise_for_status()
        data = r.json()
        text = data.get("candidates", [{}])[0].get("content", {}).get("parts", [{}])[0].get("text", "")
        return {"response": text}

async def call_llm(kind: str, prompt: str, **kwargs) -> dict:
    timeout_s = config.LLM_TIMEOUT
    model = config.OLLAMA_MODEL_CLASSIFY if kind == "classify" else config.OLLAMA_MODEL_GENERATE
    try:
        return await call_ollama(model=model, prompt=prompt, timeout_s=timeout_s, **kwargs)
    except (asyncio.TimeoutError, httpx.HTTPError, Exception) as e:
        print({"phase": "llm_call", "provider": "ollama", "status": "error", "fallback": "gemini", "reason": str(e)})
        return await call_gemini(model=config.GEMINI_MODEL, prompt=prompt, timeout_s=timeout_s, **kwargs)
```

### 4) T√≠ch h·ª£p

- Thay g·ªçi `call_ollama()` trong runtime b·∫±ng `call_llm()`.
- Gi·ªØ nguy√™n `call_ollama()` ƒë·ªÉ kh√¥ng ph√° test c≈©.

---

## ‚úÖ Ti√™u ch√≠ ch·∫•p nh·∫≠n

1. `services/main-api/.env.example` c√≥ `GEMINI_API_KEY`, `GEMINI_MODEL`, `LLM_TIMEOUT`.
2. `config_loader` ƒë·ªçc ƒë∆∞·ª£c bi·∫øn m·ªõi, API key r·ªóng kh√¥ng g√¢y crash.
3. Ollama timeout/l·ªói ‚Üí fallback Gemini ho·∫°t ƒë·ªông, log l√Ω do.
4. C·∫£ hai l·ªói ‚Üí tr·∫£ 5xx, ti·∫øp t·ª•c ch·∫°y.
5. Test c≈© pass, test m·ªõi fallback pass.

---

## üìö K·∫ø ho·∫°ch tri·ªÉn khai

1. **C·∫•u h√¨nh**:
   - Th√™m `GEMINI_API_KEY`, `GEMINI_MODEL`, `LLM_TIMEOUT` v√†o `.env.example`.
   - C·∫≠p nh·∫≠t file `.env` th·ª±c t·∫ø khi tri·ªÉn khai.
2. **Code**:
   - S·ª≠a `config_loader.py` th√™m bi·∫øn m·ªõi.
   - Vi·∫øt h√†m `call_gemini()` v√† `call_llm()` trong `main.py`.
   - S·ª≠a c√°c ƒëi·ªÉm g·ªçi model ƒë·ªÉ d√πng `call_llm()`.
3. **Test**:
   - Vi·∫øt test mock timeout Ollama ‚Üí fallback Gemini.
   - ƒê·∫£m b·∫£o test c≈© kh√¥ng b·ªã ·∫£nh h∆∞·ªüng.
4. **Tri·ªÉn khai**:
   - Build l·∫°i `main-api` b·∫±ng Docker Compose.
   - Kh·ªüi ƒë·ªông l·∫°i d·ªãch v·ª•.
5. **Gi√°m s√°t**:
   - Theo d√µi log ƒë·ªÉ x√°c nh·∫≠n fallback ho·∫°t ƒë·ªông.

---

## üöÄ Rollout¬†

```bash
docker compose -f infra/pvm/docker-compose.dev.yml build main-api
docker compose -f infra/pvm/docker-compose.dev.yml up -d main-api
```



